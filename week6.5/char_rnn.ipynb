{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import os\n",
    "#thanks @keskarnitish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate names\n",
    "* Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train NN instead.\n",
    "* Dataset contains ~8k human names from different cultures[in latin transcript]\n",
    "* Objective (toy problem): learn a generative model over names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_token = \" \"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token+name for name in names]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples =  7944\n",
      " Abagael\n",
      " Claresta\n",
      " Glory\n",
      " Liliane\n",
      " Prissie\n",
      " Geeta\n",
      " Giovanne\n",
      " Piggy\n"
     ]
    }
   ],
   "source": [
    "print ('n samples = ',len(names))\n",
    "for x in names[::1000]:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens =  55\n"
     ]
    }
   ],
   "source": [
    "#all unique characters go here\n",
    "tokens = set(''.join(names))\n",
    "\n",
    "tokens = list(tokens)\n",
    "print ('n_tokens = ',len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!token_to_id = <dictionary of symbol -> its identifier (index in tokens list)>\n",
    "token_to_id = {t: i for i,t in enumerate(tokens) }\n",
    "\n",
    "#!id_to_token = < dictionary of symbol identifier -> symbol itself>\n",
    "id_to_token = dict(enumerate(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngCQ1zBCQpIb9P1cUm6p4rRd2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f200aa0dd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len,names)),bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate names longer than MAX_LEN characters. \n",
    "MAX_LEN = 10\n",
    "\n",
    "#you will likely need to change this for any dataset different from \"names\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast everything from symbols into identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_ix = list(map(lambda name: list(map(token_to_id.get,name)),names))\n",
    "\n",
    "\n",
    "#crop long names and pad short ones\n",
    "for i in range(len(names_ix)):\n",
    "    names_ix[i] = names_ix[i][:MAX_LEN] #crop too long\n",
    "    \n",
    "    if len(names_ix[i]) < MAX_LEN:\n",
    "        names_ix[i] += [token_to_id[\" \"]]*(MAX_LEN - len(names_ix[i])) #pad too short\n",
    "        \n",
    "assert len(set(map(len,names_ix)))==1\n",
    "\n",
    "names_ix = np.array(names_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequencea','int32')\n",
    "target_values = T.matrix('actual next token','int32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN\n",
    "\n",
    "You will be building a model that takes token sequence and predicts next token\n",
    "\n",
    "\n",
    "* iput sequence\n",
    "* one-hot / embedding\n",
    "* recurrent layer(s)\n",
    "* otput layer(s) that predict output probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer,DenseLayer,EmbeddingLayer\n",
    "from lasagne.layers import RecurrentLayer,LSTMLayer,GRULayer,CustomRecurrentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "\n",
    "#!<Your neural network>\n",
    "l_emb = EmbeddingLayer(l_in, len(tokens), 8)\n",
    "\n",
    "l_rnn = GRULayer(l_emb, 32)\n",
    "\n",
    "#flatten batch and time to be compatible with feedforward layers (will un-flatten later)\n",
    "l_rnn_flat = lasagne.layers.reshape(l_rnn, (-1,l_rnn.output_shape[-1]))\n",
    "\n",
    "l_out = lasagne.layers.DenseLayer(l_rnn_flat, len(tokens), nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W_in_to_updategate, W_hid_to_updategate, b_updategate, W_in_to_resetgate, W_hid_to_resetgate, b_resetgate, W_in_to_hidden_update, W_hid_to_hidden_update, b_hidden_update, W, b]\n"
     ]
    }
   ],
   "source": [
    "# Model weights\n",
    "weights = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_output = lasagne.layers.get_output(l_out)\n",
    "#If you use dropout do not forget to create deterministic version for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_probabilities_flat = network_output\n",
    "correct_answers_flat = target_values.ravel()\n",
    "\n",
    "\n",
    "loss = lasagne.objectives.categorical_crossentropy(predicted_probabilities_flat, correct_answers_flat).mean()\n",
    "\n",
    "updates = lasagne.updates.adam(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#training\n",
    "train = theano.function([input_sequence, target_values], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "#computing loss without training\n",
    "compute_cost = theano.function([input_sequence, target_values], loss, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation\n",
    "\n",
    "Simple: \n",
    "* get initial context(seed), \n",
    "* predict next token probabilities,\n",
    "* sample next token, \n",
    "* add it to the context\n",
    "* repeat from step 2\n",
    "\n",
    "You'll get a more detailed info on how it works in the homework section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile the function that computes probabilities for next token given previous text.\n",
    "\n",
    "#reshape back into original shape\n",
    "next_word_probas = network_output.reshape((input_sequence.shape[0],input_sequence.shape[1],len(tokens)))\n",
    "#predictions for next tokens (after sequence end)\n",
    "last_word_probas = next_word_probas[:,-1]\n",
    "probs = theano.function([input_sequence],last_word_probas,allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_sample(seed_phrase=None,N=MAX_LEN,t=1,n_snippets=1):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "        \n",
    "    parameters:\n",
    "        sample_fun - max_ or proportional_sample_fun or whatever else you implemented\n",
    "        \n",
    "        The phrase is set using the variable seed_phrase\n",
    "\n",
    "        The optional input \"N\" is used to set the number of characters of text to predict.     \n",
    "    '''\n",
    "    if seed_phrase is None:\n",
    "        seed_phrase=start_token\n",
    "    if len(seed_phrase) > MAX_LEN:\n",
    "        seed_phrase = seed_phrase[-MAX_LEN:]\n",
    "    assert type(seed_phrase) is str\n",
    "\n",
    "    snippets = []\n",
    "    for _ in range(n_snippets):\n",
    "        sample_ix = []\n",
    "        x = list(map(lambda c: token_to_id.get(c,0), seed_phrase))\n",
    "        x = np.array([x])\n",
    "\n",
    "        for i in range(N):\n",
    "            # Pick the character that got assigned the highest probability\n",
    "            p = probs(x).ravel()\n",
    "            p = p**t / np.sum(p**t)\n",
    "            ix = np.random.choice(np.arange(len(tokens)),p=p)\n",
    "            sample_ix.append(ix)\n",
    "\n",
    "            x = np.hstack((x[-MAX_LEN+1:],[[ix]]))\n",
    "\n",
    "        random_snippet = seed_phrase + ''.join(id_to_token[ix] for ix in sample_ix)    \n",
    "        snippets.append(random_snippet)\n",
    "        \n",
    "    print(\"----\\n %s \\n----\" % '; '.join(snippets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "Here you can tweak parameters or insert your generation function\n",
    "\n",
    "\n",
    "__Once something word-like starts generating, try increasing seq_length__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    \n",
    "    rows = data[np.random.randint(0,len(data),size=batch_size)]\n",
    "    \n",
    "    return rows[:,:-1],rows[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated names\n",
      "----\n",
      "  Con       ;  Siderde   ;  Luletenne ;  Jalolied  ;  Bancy     ;  Emmie     ;  Grenn     ;  Rothan    ;  Pinetta   ;  Caristi    \n",
      "----\n",
      "Epoch 41 average loss = 1.5632817339897156\n",
      "Generated names\n",
      "----\n",
      "  Nar       ;  Kiamar    ;  Bionabret ;  Cleag     ;  Belbem    ;  Jerre     ;  Owine     ;  Manda     ;  Jeznie    ;  Elli       \n",
      "----\n",
      "Epoch 42 average loss = 1.549780816555023\n",
      "Generated names\n",
      "----\n",
      "  Isellah   ;  Jonnie    ;  Sibelee   ;  Marah     ;  Dadmy     ;  Tsiy      ;  Allite    ;  Marchgabel;  Caraba    ;  Silv       \n",
      "----\n",
      "Epoch 43 average loss = 1.5553604907989502\n",
      "Generated names\n",
      "----\n",
      "  Mudinue   ;  Jocert    ;  Althert   ;  Elisit    ;  Sticord   ;  Dody      ;  Jimira    ;  Lis       ;  Elaja     ;  Ladesho    \n",
      "----\n",
      "Epoch 44 average loss = 1.5484518339633941\n",
      "Generated names\n",
      "----\n",
      "  Merley    ;  Cadier    ;  Jonetta   ;  Aloire    ;  Law       ;  Oryth     ;  Mor       ;  Janeline  ;  Rapheldert;  Deomendos  \n",
      "----\n",
      "Epoch 45 average loss = 1.5530347924232484\n",
      "Generated names\n",
      "----\n",
      "  Ellee     ;  Janbel    ;  Aemryo    ;  Galna     ;  Wayra     ;  Trizenna  ;  Luzett    ;  Viline    ;  Shelia    ;  Arriet     \n",
      "----\n",
      "Epoch 46 average loss = 1.551792647600174\n",
      "Generated names\n",
      "----\n",
      "  Ilsie     ;  Sellard   ;  Dila      ;  Eshana    ;  Meojglolen;  Holon     ;  Luzes     ;  Jaane     ;  Lena      ;  Laelit     \n",
      "----\n",
      "Epoch 47 average loss = 1.5481172094345093\n",
      "Generated names\n",
      "----\n",
      "  Maroda    ;  Zathia    ;  Grandricka;  Charla    ;  Reac      ;  Caryli    ;  Arnetta   ;  Porl      ;  Sibinda   ;  Doni       \n",
      "----\n",
      "Epoch 48 average loss = 1.5423025076389312\n",
      "Generated names\n",
      "----\n",
      "  Nena      ;  Brigulie  ;  Bar       ;  Charemarin;  Heliz     ;  Elinia    ;  Aphall    ;  Shonny    ;  Leena     ;  Lina       \n",
      "----\n",
      "Epoch 49 average loss = 1.542398952960968\n"
     ]
    }
   ],
   "source": [
    "print(\"Training ...\")\n",
    "\n",
    "\n",
    "#total N iterations\n",
    "n_epochs=50\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 500\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=20\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    print( \"Generated names\")\n",
    "    generate_sample(n_snippets=10)\n",
    "\n",
    "    avg_cost = 0;\n",
    "    \n",
    "    for _ in range(batches_per_epoch):\n",
    "        \n",
    "        x,y = sample_batch(names_ix,batch_size)\n",
    "        avg_cost += train(x, y)\n",
    "        \n",
    "    print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "    if epoch % 10 == 0:\n",
    "        clear_output(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  Dadola    ;  Eis       ;  Ala-Jorol ;  Freddy    ;  Horrie    ;  Harsie    ;  Lones     ;  Idrera    ;  Velina    ;  Mavi      ;  Gerin     ;  Flid      ;  Barille   ;  Bellaim   ;  Cop       ;  Angwas    ;  Rechelen  ;  Milesta   ;  Corth     ;  Kar       ;  Cari      ;  Roril     ;  Huliquia  ;  Vogy      ;  Kisti     ;  Bid       ;  Rosolia   ;  Noladi    ;  Ryzoten   ;  Ches      ;  Lean      ;  Merricka  ;  Banti     ;  Elivia    ;  Hysti     ;  Syb       ;  Span      ;  Cersia    ;  Tavervi   ;  Groeter   ;  Sherlee   ;  Kridtan   ;  Ferray    ;  Lunette   ;  Helone    ;  Mairre    ;  Roda      ;  Clais     ;  Dannita   ;  Anille    ;  Lina      ;  Elgelince ;  Shelmi    ;  Erwi      ;  Charly    ;  Xina      ;  Tryn      ;  Jomin     ;  Nivalah   ;  Kaedie    ;  Catham    ;  Rachickell;  Tilwerd   ;  Sheena    ;  Givianne  ;  Melissine ;  Margeal   ;  Ephell    ;  Grances   ;  Maren-Jann;  Cheana    ;  Hullyn    ;  Katus     ;  Terra     ;  Jocos     ;  Potlaby   ;  Hilorio   ;  Alonk     ;  Miri      ;  Cathelyne ;  Ter       ;  Kariss    ;  Sie       ;  Vegsa     ;  LaZ       ;  Christilie;  Delie     ;  Javinda   ;  Histie    ;  Garlond   ;  Lunilla   ;  Korke     ;  Geloigp   ;  Grend     ;  Ana       ;  Sobfristie;  Konalin   ;  Mariah    ;  Ola       ;  Madrita    \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "generate_sample(n_snippets=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  Mironi         ;  Mironina       ;  Miron          ;  Mironah        ;  Mirone         ;  Mironie        ;  Mirona         ;  Mironey        ;  Mirona         ;  Mirono          \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "generate_sample(seed_phrase=\" Miron\",n_snippets=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework part 1 - generate questions (4 pts)\n",
    "\n",
    "* Apply recurrent neural networks to generate human-readable questions.\n",
    "* The dataset origins from https://www.kaggle.com/c/quora-question-pairs - a recent kaggle challenge.\n",
    "* The code below shows how to read the dataset\n",
    "* Please download the __train dataset__ from [here](https://www.kaggle.com/c/quora-question-pairs/data)\n",
    "* Avoid using test dataset as it contains artificially generated data.\n",
    "* Alternatively, pick any similar dataset you like.\n",
    "\n",
    "### [bonus] Word-level model (4+ points)\n",
    "\n",
    "Learn to generate questions on _word_ level, generating one word per RNN iteration.\n",
    "\n",
    "Kudos for \n",
    "* pre-training embedding layer with word2vec or similar\n",
    "* using more compute-efficient softmax functions (hierarchical or sampled softmax)\n",
    "* anything creative :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import os\n",
    "#thanks @keskarnitish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples =  537361\n",
      " Does the band Tool have any love songs?;\n",
      " Should I go for executive MBA or consider switching job?;\n",
      " Where can I find a happy ending in Brooklyn?;\n",
      " Who is responsible for partition of India in 1947?;\n",
      " How did you meet your significant other?;\n",
      " Can you spy on a cellphone without having to download software to the target phone?;\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./train.csv\")\n",
    "df = pd.concat([df.question1,df.question2])\n",
    "\n",
    "questions = list(set(df))\n",
    "questions = filter(lambda x: type(x) is str, questions)\n",
    "start_token,end_token = \" \",\";\"\n",
    "questions = [start_token+name.replace(\";\",\",\")+end_token for name in questions]\n",
    "\n",
    "print('n samples = ',len(questions))\n",
    "for x in questions[::100000]:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = [question.encode('utf-8') for question in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = [list(map(chr, question)) for question in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens =  198\n"
     ]
    }
   ],
   "source": [
    "tokens = set()\n",
    "for question in questions:\n",
    "    tokens.update(question)\n",
    "\n",
    "tokens = list(tokens)\n",
    "print ('n_tokens = ',len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!token_to_id = <dictionary of symbol -> its identifier (index in tokens list)>\n",
    "token_to_id = {t: i for i,t in enumerate(tokens) }\n",
    "\n",
    "#!id_to_token = < dictionary of symbol identifier -> symbol itself>\n",
    "id_to_token = {i: t for i,t in enumerate(tokens) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZvJygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff86d80c978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(list(map(len,questions)),bins=25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate names longer than MAX_LEN characters. \n",
    "MAX_LEN = 100\n",
    "\n",
    "#you will likely need to change this for any dataset different from \"names\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast everything from symbols into identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions_ix = list(map(lambda question: list(map(token_to_id.get, question)),questions))\n",
    "\n",
    "\n",
    "#crop long names and pad short ones\n",
    "for i in range(len(questions_ix)):\n",
    "    questions_ix[i] = questions_ix[i][:MAX_LEN] #crop too long\n",
    "    \n",
    "    if len(questions_ix[i]) < MAX_LEN:\n",
    "        questions_ix[i] += [token_to_id[\" \"]]*(MAX_LEN - len(questions_ix[i])) #pad too short\n",
    "        \n",
    "assert len(set(map(len,questions_ix))) == 1\n",
    "\n",
    "questions_ix = np.array(questions_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('token sequence','int32')\n",
    "target_values = T.matrix('actual next token','int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer,DenseLayer,EmbeddingLayer\n",
    "from lasagne.layers import RecurrentLayer,LSTMLayer,GRULayer,CustomRecurrentLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer(shape=(None, None),input_var=input_sequence)\n",
    "l_emb = EmbeddingLayer(l_in, len(tokens), 8)\n",
    "l_rnn = GRULayer(l_emb, 32)\n",
    "l_rnn_flat = lasagne.layers.reshape(l_rnn, (-1,l_rnn.output_shape[-1]))\n",
    "l_out = lasagne.layers.DenseLayer(l_rnn_flat, len(tokens), nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W_in_to_updategate, W_hid_to_updategate, b_updategate, W_in_to_resetgate, W_hid_to_resetgate, b_resetgate, W_in_to_hidden_update, W_hid_to_hidden_update, b_hidden_update, W, b]\n"
     ]
    }
   ],
   "source": [
    "# Model weights\n",
    "weights = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_output = lasagne.layers.get_output(l_out)\n",
    "#If you use dropout do not forget to create deterministic version for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_probabilities_flat = network_output\n",
    "correct_answers_flat = target_values.ravel()\n",
    "\n",
    "loss = lasagne.objectives.categorical_crossentropy(predicted_probabilities_flat, correct_answers_flat).mean()\n",
    "\n",
    "updates = lasagne.updates.adam(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "train = theano.function([input_sequence, target_values], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "#computing loss without training\n",
    "compute_cost = theano.function([input_sequence, target_values], loss, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "next_word_probas = network_output.reshape((input_sequence.shape[0],input_sequence.shape[1],len(tokens)))\n",
    "last_word_probas = next_word_probas[:,-1]\n",
    "probs = theano.function([input_sequence],last_word_probas,allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(seed_phrase=None,N=MAX_LEN,t=1,n_snippets=1):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "        \n",
    "    parameters:\n",
    "        sample_fun - max_ or proportional_sample_fun or whatever else you implemented\n",
    "        \n",
    "        The phrase is set using the variable seed_phrase\n",
    "\n",
    "        The optional input \"N\" is used to set the number of characters of text to predict.     \n",
    "    '''\n",
    "    if seed_phrase is None:\n",
    "        seed_phrase=start_token\n",
    "    if len(seed_phrase) > MAX_LEN:\n",
    "        seed_phrase = seed_phrase[-MAX_LEN:]\n",
    "    assert type(seed_phrase) is str\n",
    "\n",
    "    snippets = []\n",
    "    for _ in range(n_snippets):\n",
    "        sample_ix = []\n",
    "        x = list(map(lambda c: token_to_id.get(c,0), seed_phrase))\n",
    "        x = np.array([x])\n",
    "\n",
    "        for i in range(N):\n",
    "            # Pick the character that got assigned the highest probability\n",
    "            p = probs(x).ravel()\n",
    "            p = p**t / np.sum(p**t)\n",
    "            ix = np.random.choice(np.arange(len(tokens)),p=p)\n",
    "            sample_ix.append(ix)\n",
    "\n",
    "            x = np.hstack((x[-MAX_LEN+1:],[[ix]]))\n",
    "\n",
    "        random_snippet = seed_phrase + ''.join(id_to_token[ix] for ix in sample_ix)    \n",
    "        snippets.append(random_snippet)\n",
    "        \n",
    "    print(\"----\\n %s \\n----\" % '; '.join(snippets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size):\n",
    "    \n",
    "    rows = data[np.random.randint(0,len(data),size=batch_size)]\n",
    "    \n",
    "    return rows[:,:-1],rows[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Training ...\")\n",
    "\n",
    "\n",
    "#total N iterations\n",
    "n_epochs=10\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 500\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=10\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    print( \"Generated questions\")\n",
    "    generate_sample(n_snippets=2)\n",
    "\n",
    "    avg_cost = 0;\n",
    "    \n",
    "    for _ in range(batches_per_epoch):\n",
    "        \n",
    "        x,y = sample_batch(questions_ix,batch_size)\n",
    "        avg_cost += train(x, y)\n",
    "        \n",
    "    print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))\n",
    "    if epoch % 5 == 0:\n",
    "        clear_output(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " Is it possible to get a job in In ? ;                            \n",
      " What are the best ways to improve the English skills ? ;                          \n",
      "----\n",
      "----\n",
      " What is the best way to get a job in the US ? ;                      \n",
      " What is the best way to get a job in a foreign country ? ;                    \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "generate_sample(n_snippets=2, t=2)\n",
    "generate_sample(n_snippets=2, t=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При более высокой температуре вопросы получаются отличными"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
